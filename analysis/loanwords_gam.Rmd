---
title: "Cognitive influences in language evolution: English data"
output:
  pdf_document:
    toc: true
---

# Introduction

This is the model code for Monaghan & Roberts, "Cognitive influences in language evolution: Psycholinguistic predictors of loan word borrowing".  It takes data from the WOLD database of borrowing for English and tries to predict whether a word has been borrowed or not according to various psycholinguitic measures.

The main fields in the data frame are:

-  word: Orthographic form
-  borrowing: variable from WOLD indicating level of evidence for borrowing:
  -  1 = definately borrowed
  -  5 = no evidence of borrowing
-  age_oldest, age_youngest: Dates from WOLD indicating estiamte of data of entry into English
-  phonology:  Phonological form
-  phonlength:  Number of segments in the phonological form
-  AoA: Age of acquisition ratings from Kuperman, Stadthagen-Gonzalez, and Brysbaert (2012).
-  AoA_obj: Objective, test-based age of acuqisition from Brysbaert & Biemiller (2017)
-  subtlexzipf:  Log frequency of word from the SUBTLEX database
-  conc:  Concreteness ratings from Brysbaert, Warriner, & Kuperman (2014)
-  cat: Dominant part of speech according to SUBTLEX.
-  bor15:  Conversion of the WOLD borrowing variable into a numeric (0 = not borrowed, 1 = borrowed)

## A note on EDF values and random effects

The Estimated Degrees of Freedom (EDF) is an indication of how non-linear a smooth term is (higher = less linear). It is intended as a diagnostic measure of the shape of the curve, rather than a value used in estimating significance. The EDF is not the same as a simple polynomial curve’s degree. Instead of a single polynomial curve, each smooth term is a collection of underlying basis functions (simpler curves). When each basis function is weighted by a coefficient, they add up to fit the data. The model attempts to find a collection of basis functions and weighting coefficients that add up to fit the data. Smaller collections of basis functions (simpler models) are preferred and larger (more complex) collections are penalised.

When the model converges on a solution, each smooth term is a collection of simpler curves. Each curve might have a polynomial degree, but it is also useful to have an estimate of the linearity/non-linearity of the whole smooth term.  This is what the EDF provides: an EDF of 1 indicates a linear relationship, and higher values indicate more non-linear relationship. The definition of EDF in the implementaion we use is described in Wood (2008):

> "Associated with each smooth function is one or more measures of function 'wiggliness' $\beta_T^j \tilde{S}_j \beta_j$ where $\tilde{S}_j$
is a matrix of known coefficients. Typically the wiggliness measure evaluates something like the univariate spline penalty."

That is, an EDF value is a combination of non-linearity measures of the basis functions, weighted by the weighting coefficient of each basis function. So, in general, a curve with an EDF of around 2 will look like a quadratic curve, and an EDF of around 3 will look like a cubic curve. However, this does not have to be the case: a smooth term could have a strong linear term, and a very weak non-linear term. The EDF captures this possibility as a continuous value. The simplest way to actually assess the smooth term is to plot it.

Random effects in the GAM implementation we use are treated just like a smooth term with the identity matrix as the penalty coefficient matrix. When entering part of speech as a random (intercept) effect, coefficients are created for each part of speech, modelled as independent and identically distributed normal random variables. The values are defined as discrete points along a smooth function. So, just like in a mixed effects model, the probability of borrowing can be adjusted by a random intercept (the coefficients), e.g. the model can represent nouns as having a higher probability of borrowing, adjectives as slightly less probable and so on. Stronger differences between levels of the random effect would need be represented by more complex functions, which would be penalised (similar to how a linear mixed effect model penalises random effect coefficient estimates which deviate from a normal distribution). The EDF value for the random effects relates to the ‘wiggliness’ of these coefficients when plotted in a regular space. This makes the EDF difficult to interpret.  A random effect where there were no differences between levels would have an EDF of 1 (a flat line), but it would also be 1 when there were consistent distances between each level. So a high EDF would indicate something like an imbalance in the distribution of coefficients. i.e. a few parts of speech are very likely to be borrowed, and most are very unlikely. This is in fact what we have, as shown in table 2 of the manuscript, and is consistent with previous studies of the effect of borrowing relating to grammatical category.


\newpage

```{r echo=F,eval=F}
setwd("~/Documents/MPI/MonaghanAoA/Stats 2/analysis/")
```


# Load libraries

```{r warning=F, message=F}
library(mgcv)
library(sjPlot)
library(lattice)
library(ggplot2)
library(dplyr)
library(party)
library(lmtest)
library(gridExtra)
library(scales)
library(itsadug)
library(ggfortify)
library(factoextra)
library(gridExtra)
library(reshape2)
library(binom)

logit2per = function(X){
  return(exp(X)/(1+exp(X)))
}

rescaleGam = function(px, n, xvar, xlab="",breaks=NULL,xlim=NULL){
  y = logit2per(px[[n]]$fit)
  x = px[[n]]$x *attr(xvar,"scaled:scale") + attr(xvar,"scaled:center")
  se.upper = logit2per(px[[n]]$fit+px[[n]]$se)
  se.lower = logit2per(px[[n]]$fit-px[[n]]$se)
  dx = data.frame(x=x,y=y,ci.upper=se.upper,ci.lower=se.lower)
  plen = ggplot(dx, aes(x=x,y=y))+
    geom_ribbon(aes(ymin=ci.lower,ymax=ci.upper), alpha=0.3)+
    geom_line(size=0.5,linetype=3) +
    xlab(xlab)+
    ylab("Probability of borrowing")
  if(!is.null(breaks)){
    plen = plen + scale_x_continuous(breaks = breaks)
  }
  if(!is.null(xlim)){
   plen = plen + coord_cartesian(ylim = c(0,1),xlim=xlim)
  } else{
    plen = plen + coord_cartesian(ylim = c(0,1))
  }
  return(plen)
}
# Code for assessing significance of GAM slopes
source("GAM_derivaties.R")

```

# Load data

```{r}
dataloan <- read.csv("../data/loanword9.csv",stringsAsFactors = F)
dataloan$bor15 <- ifelse(dataloan$borrowing==1,1, ifelse(dataloan$borrowing==5,0,NA))
dataloan$bor15.cat <- factor(dataloan$bor15)
```

Convert to numbers.

```{r warning=F}
dataloan$subtlexzipf = as.numeric(dataloan$subtlexzipf)
dataloan$AoA = as.numeric(dataloan$AoA)
dataloan$conc = as.numeric(dataloan$conc)

aoaSD = sd(dataloan$AoA,na.rm = T)
aoaMean = mean(dataloan$AoA/aoaSD,na.rm=T)
dataloan$cat = factor(dataloan$cat)
```

Select only complete cases.

```{r}
dataloan2 = dataloan[complete.cases(dataloan[,
               c("phonlength","AoA",
               "subtlexzipf", "cat",
               'conc','bor15')]),]
```

Scale and center:

```{r}
dataloan2$AoAscale <- scale(dataloan2$AoA)

dataloan2$subtlexzipfscale <- scale(dataloan2$subtlexzipf)

phonlength.center = median(dataloan2$phonlength)
dataloan2$phonlengthscale <-
  dataloan2$phonlength - phonlength.center
phonlength.scale = sd(dataloan2$phonlengthscale)
dataloan2$phonlengthscale = dataloan2$phonlengthscale/phonlength.scale

attr(dataloan2$phonlengthscale,"scaled:scale") = phonlength.scale
attr(dataloan2$phonlengthscale,"scaled:center") = phonlength.center

dataloan2$concscale <- scale(dataloan2$conc)
conc.scale = attr(dataloan2$concscale,"scaled:scale")
conc.center = attr(dataloan2$concscale,"scaled:center")

dataloan2$cat = relevel(dataloan2$cat,"Noun")

dataloan2$AoA_objscaled = scale(dataloan2$AoA_obj)


dataloan2$source.language[dataloan2$bor15==0] = "English"
dataloan2$source.language = factor(dataloan2$source.language)

dataloan2$SLMWL = scale(log(dataloan2$source.language.mean.word.length))
dataloan2$SWF = scale(dataloan2$source.language.word.freq)
```


Identify Swadesh words:

```{r}
swd = read.csv("../data/SwadeshConcepts.txt", header = F, stringsAsFactors = F)$V1
dataloan2$Swadesh = dataloan2$word %in% swd
```


\newpage

# Plots

Raw data

```{r}
dataloan2$Borrowed = c("Not borrowed", "Borrowed")[dataloan2$bor15+1]
ggplot(dataloan2[!is.na(dataloan2$Borrowed),], aes(x=AoA, colour=Borrowed)) +
  geom_density()

ggplot(dataloan2[!is.na(dataloan2$Borrowed),], aes(x=as.numeric(subtlexzipfscale), colour=Borrowed)) +
  geom_density()
  
ggplot(dataloan2[!is.na(dataloan2$Borrowed),], aes(x=as.numeric(subtlexzipf), y=bor15)) +
  stat_smooth()+
  xlab("Frequency")

ggplot(dataloan2[!is.na(dataloan2$Borrowed),], aes(x=as.numeric(phonlength), y=bor15)) +
  stat_smooth() +
  xlab("Length")
  
dataloan2$subtlexzipf.cat = cut(
  dataloan2$subtlexzipf, 
  breaks = quantile(dataloan2$subtlexzipf,
                    prob=seq(0,1,length.out=4)),
  include.lowest = T)

```

Look at variation between parts of speech.  We calculate the means, but the number of observations is very different for each category.  We estimate confidence intervals around the mean with Wilson's binomial confidence interval method.

```{r}
catx = data.frame(
  PoS = tapply(dataloan2$cat, dataloan2$cat, function(X){as.character(X[1])}),
  mean = tapply(dataloan2$bor15, dataloan2$cat, mean),
  n = tapply(dataloan2$bor15, dataloan2$cat, length),
  confint = binom.confint(
    tapply(dataloan2$bor15, dataloan2$cat, sum),
    tapply(dataloan2$bor15, dataloan2$cat, length),
    methods="wilson"
  )
)
catx = catx[order(catx$confint.lower, decreasing = T),]
catx$PoS = factor(catx$PoS, levels = catx[order(catx$confint.lower, decreasing = T),]$PoS)

posg = ggplot(catx, aes(x=mean, y=PoS)) +
  geom_point(size=2) + 
  ylab("Part of speech") +
  xlab("Proportion of words borrowed")+
  scale_x_continuous(labels=percent_format()) +
  geom_text(aes(label=n), nudge_y=0.4) + 
  geom_errorbarh(aes(xmin=confint.lower, xmax=confint.upper))
posg
pdf("../results/graphs/POS_Borrowing.pdf",
    width = 6,
    height = 4)
posg
dev.off()

catx$mean= catx$mean*100
catx$confint.lower= catx$confint.lower*100
catx$confint.upper= catx$confint.upper*100
write.csv(catx[,c("PoS","mean",
                  'n','confint.lower','confint.upper')], 
    "../results/English_POS_BorrowingProportions.csv", 
    row.names = F)

```


\newpage

# GAM 

```{r}
m0 = bam(bor15.cat ~
      s(phonlengthscale) + 
      s(AoAscale) + 
      s(subtlexzipfscale) +
      s(concscale) +
      s(cat,bs='re')+
      s(cat,phonlengthscale,bs='re')+
      s(cat,AoAscale,bs='re')+
      s(cat,subtlexzipfscale,bs='re')+
      s(cat,concscale,bs='re'),
    data = dataloan2,
    family='binomial')
```

```{r}
summary(m0)
```

## Interactions

Test whether an interaction between AoA and frequency is warranted using likelihood ratio comparisons:

```{r}
m1 = bam(bor15.cat ~
      s(phonlengthscale) + 
      s(AoAscale) + 
      s(subtlexzipfscale) +
      s(concscale) +
      s(cat,bs='re')+
      s(cat,phonlengthscale,bs='re')+
      s(cat,AoAscale,bs='re')+
      s(cat,subtlexzipfscale,bs='re')+
      s(cat,concscale,bs='re') +
      te(AoAscale,subtlexzipfscale),
    data = dataloan2,
    family='binomial')

lrtest(m0,m1)
```

No significant improvement.

Test whether an interaction between AoA and length is warranted:

```{r}
m2 = bam(bor15.cat ~
      s(phonlengthscale) + 
      s(AoAscale) + 
      s(subtlexzipfscale) +
      s(concscale) +
      s(cat,bs='re')+
      s(cat,phonlengthscale,bs='re')+
      s(cat,AoAscale,bs='re')+
      s(cat,subtlexzipfscale,bs='re')+
      s(cat,concscale,bs='re') +
      te(AoAscale,phonlengthscale),
    data = dataloan2,
    family='binomial')

lrtest(m0,m2)
```

The `lrtest` function above suggests a significant improvement, but actually the log likelihood has not changed:
```{r}
logLik(m0)
logLik(m2)
```

Therefore, there is no improvement and we should prefer the simpler model (without the interaction).

Test whether an interaction between Frequency and length is warranted:

```{r}
m3 = bam(bor15.cat ~
      s(phonlengthscale) + 
      s(AoAscale) + 
      s(subtlexzipfscale) +
      s(concscale) +
      s(cat,bs='re')+
      s(cat,phonlengthscale,bs='re')+
      s(cat,AoAscale,bs='re')+
      s(cat,subtlexzipfscale,bs='re')+
      s(cat,concscale,bs='re') +
      te(subtlexzipfscale,phonlengthscale),
    data = dataloan2,
    family='binomial')

lrtest(m0,m3)
```

No significant improvement.

So no interactions are necessary.

\newpage

# Model plots

Plot the model estimates, changing the dependent scale to probability and the independent variables to their original scales. This code is hidden, but you can view it in the Rmd file.

```{r echo=F, fig.show="hide"}
# Code for plotting model estimates 
px = plot.gam(m0,select=1, xlab="Word length", ylab="Log odds of borrowing",shade = T)
y = logit2per(px[[1]]$fit)
x = px[[1]]$x *phonlength.scale + phonlength.center
se.upper = logit2per(px[[1]]$fit+px[[1]]$se)
se.lower = logit2per(px[[1]]$fit-px[[1]]$se)
dx = data.frame(x=x,y=y,ci.upper=se.upper,ci.lower=se.lower)
plen = ggplot(dx, aes(x=x,y=y))+
  geom_ribbon(aes(ymin=ci.lower,ymax=ci.upper), alpha=0.3)+
  geom_line(size=0.5,linetype=3) +
  scale_x_continuous(breaks = c(2,4,6,8,10))+
  xlab("Word Length")+
  ylab("Probability of borrowing")+
  coord_cartesian(ylim = c(0,1),xlim = c(2,10)) + ggtitle("English")

#Age of acquisition, Frequency and Concreteness.

paoa = rescaleGam(px,2,dataloan2$AoAscale, "Age of acquisition",
                  xlim=c(1.5,14.5),breaks=c(2,4,6,8,10,12,14)) +ggtitle("")
pfreq = rescaleGam(px,3,dataloan2$subtlexzipfscale, "Frequency",
                   xlim = c(1,12), breaks=c(2,4,6,8,10,12)) +ggtitle("")
pconc = rescaleGam(px,4,dataloan2$concscale, "Concreteness")+ ggtitle("English")
```

```{r echo=F}
pconc
grid.arrange(plen,pfreq,paoa, nrow=1)
pdf(file='../results/graphs/English_ModelResults.pdf',
    height =3,width = 8)
grid.arrange(plen,pfreq,paoa, nrow=1)
dev.off()
```


We can also plot the effects when removing the random effects (using the library `itsadug`).  These are essentially the same, though not as easy to understand.

```{r echo=F, fig.show="hide"}
# from itsadug: plot ignoring random effects
px = plot_smooth(m0,view="subtlexzipfscale", rm.ranef = T, print.summary=F)
px$fv$fit = logit2per(px$fv$fit)
px$fv$ul = logit2per(px$fv$ul)
px$fv$ll = logit2per(px$fv$ll)
px$fv$subtlexzipfscale = px$fv$subtlexzipfscale * attr(dataloan2$subtlexzipfscale,"scaled:scale") + attr(dataloan2$subtlexzipfscale,"scaled:center")

gFreq = ggplot(px$fv, aes(x=subtlexzipfscale,y=fit)) + 
  geom_ribbon(aes(ymin=ll,ymax=ul), alpha=0.3) +
  geom_line(size=1) +
  ylab("Probability of borrowing") +
  xlab("Frequency") +
  coord_cartesian(ylim=c(0,1))

px = plot_smooth(m0,view="AoAscale", rm.ranef = T, print.summary=F)
px$fv$fit = logit2per(px$fv$fit)
px$fv$ul = logit2per(px$fv$ul)
px$fv$ll = logit2per(px$fv$ll)
px$fv$AoAscale = px$fv$AoAscale * attr(dataloan2$AoAscale,"scaled:scale") + attr(dataloan2$AoAscale,"scaled:center")

gAoA = ggplot(px$fv, aes(x=AoAscale,y=fit)) + 
  geom_ribbon(aes(ymin=ll,ymax=ul), alpha=0.3) +
  geom_line(size=1) +
  ylab("Probability of borrowing") +
  xlab("Age of acquisition")+
  coord_cartesian(ylim=c(0,1))

px = plot_smooth(m0,view="phonlengthscale", rm.ranef = T, print.summary = F)
px$fv$fit = logit2per(px$fv$fit)
px$fv$ul = logit2per(px$fv$ul)
px$fv$ll = logit2per(px$fv$ll)
px$fv$phonlengthscale = px$fv$phonlengthscale * phonlength.scale + phonlength.center

gLen = ggplot(px$fv, aes(x=phonlengthscale,y=fit)) + 
  geom_ribbon(aes(ymin=ll,ymax=ul), alpha=0.3) +
  geom_line(size=1) +
  ylab("Probability of borrowing") +
  xlab("Word length")+
  coord_cartesian(ylim=c(0,1))
```

```{r echo=F}
grid.arrange(gLen, gFreq, gAoA, nrow=1)
pdf("../results/graphs/English_ModelResults_NoRF.pdf",
    height =3,width = 8)
grid.arrange(gLen, gFreq, gAoA, nrow=1)
dev.off()
```

## Contour plots

Visualise relationships between variables.

```{r,  out.height = '80%'}
par(mfrow=c(3,2))
vis.gam(m0,view = c("AoAscale","subtlexzipfscale"),plot.type = "contour")
vis.gam(m0,view = c("AoAscale","phonlengthscale"),plot.type = "contour")
vis.gam(m0,view = c("AoAscale","concscale"),plot.type = "contour")
vis.gam(m0,view = c("subtlexzipfscale","phonlengthscale"),plot.type = "contour")
vis.gam(m0,view = c("subtlexzipfscale","concscale"),plot.type = "contour")
vis.gam(m0,view = c("phonlengthscale","concscale"),plot.type = "contour")
par(mfrow=c(1,1))
```

\newpage

## Significant trends

The plots below highlight which sections of the GAM splines are significantly increasing or decreasing.  This method comes from [https://www.fromthebottomoftheheap.net/2014/05/15/identifying-periods-of-change-with-gams/](https://www.fromthebottomoftheheap.net/2014/05/15/identifying-periods-of-change-with-gams/).  The basic idea is to calculate the derivatives of the slope (how much the slope is increasing or decreasing) and then compute confidence intervals for the derivatives from their standard errors. If the confidence intervals of the derivatives do not overlap zero, then they are considered significant.

The results suggest:

-  Frequency only significantly increasing for a sub-section of the spline
-  AoA and length significantly increasing for essentially the whole range
-  Concreteness not significantly increasing nor decreasing in any part of the curve.

```{r}
pSigFreq = plotGAMSignificantSlopes(m0,"subtlexzipfscale","Frequency")
pSigAoA =  plotGAMSignificantSlopes(m0,"AoAscale","AoA")
pSigLen =  plotGAMSignificantSlopes(m0,"phonlengthscale","length")
pSigConc = plotGAMSignificantSlopes(m0,"concscale","Concreteness")

```

```{r echo=F,fig.show="hide"}
# Rescale significant trend plots
rescaleDerivitiesPlot = function(origPlot,sigCurveData){
sigCurveData$curve.x = origPlot$data$x
sigCurveData$m2.dsig.incr = logit2per(sigCurveData$m2.dsig.incr)
sigCurveData$m2.dsig.decr = logit2per(sigCurveData$m2.dsig.decr)

 ret = origPlot + 
  geom_path(data = sigCurveData, 
            aes(x = curve.x, 
                y = m2.dsig.incr,
                size=0.9)) +
  geom_path(data = sigCurveData, 
            aes(x = curve.x, 
                y = m2.dsig.decr,
                size=0.9)) +
  theme(legend.position = 'none') 
 return(ret)
}
pSigLen2 = rescaleDerivitiesPlot(plen,pSigLen)
pSigAoA2 = rescaleDerivitiesPlot(paoa,pSigAoA)
pSigFreq2 = rescaleDerivitiesPlot(pfreq,pSigFreq)
pSigConc2 = rescaleDerivitiesPlot(pconc,pSigConc)
```

```{r echo=F}
grid.arrange(pSigLen2,pSigFreq2,pSigAoA2, nrow=1)
pdf(file='../results/graphs/English_ModelResults_withSignificantSlopesHighlighted.pdf',
    height =3,width = 8)
grid.arrange(pSigLen2,pSigFreq2,pSigAoA2, nrow=1)
dev.off()
```


\newpage

# Objective measures of AoA

Below we run the same model, but with objective, test-based AoA from Brysbaert et al. (2017).  Note that the values for objective AoA are only whole numbers, so there are not as many unique values and we have to limit the number of knots that the model uses.

```{r}
m0.obj = bam(bor15.cat ~
      s(phonlengthscale) + 
      s(AoA_objscaled, k=3) + 
      s(subtlexzipfscale) +
      s(concscale) +
      s(cat,bs='re')+
      s(cat,phonlengthscale,bs='re')+
      s(cat,AoA_objscaled,bs='re')+
      s(cat,subtlexzipfscale,bs='re')+
      s(cat,concscale,bs='re'),
    data = dataloan2[!is.na(dataloan2$AoA_objscaled),],
    family='binomial')
```

```{r}
summary(m0.obj)
```

Very similar results.  For example, almost all coefficients are the same:

```{r}
plot(m0.obj$coefficients, m0$coefficients[names(m0.obj$coefficients)])
```

The outlier is the coefficient for `subtlexzipfscale`.

And chi squared terms are similar:

```{r}
m0S = summary(m0)
m0.objS = summary(m0.obj)
cbind(m0=m0S$chi.sq,m0.obj=m0.objS$chi.sq)[1:4,]
```



## Objective AoA: Model plots

Visualise the model smooth terms, independent of influence of random effects. The code is hidden, but you can view it in the Rmd file.

```{r echo=F,fig.show='hide'}
# from itsadug: plot ignoring random effects
px = plot_smooth(m0.obj,view="subtlexzipfscale", 
                 rm.ranef = T,print.summary=F)
px$fv$fit = logit2per(px$fv$fit)
px$fv$ul = logit2per(px$fv$ul)
px$fv$ll = logit2per(px$fv$ll)
px$fv$subtlexzipfscale = px$fv$subtlexzipfscale * attr(dataloan2$subtlexzipfscale,"scaled:scale") + attr(dataloan2$subtlexzipfscale,"scaled:center")

gFreqO = ggplot(px$fv, aes(x=subtlexzipfscale,y=fit)) + 
  geom_ribbon(aes(ymin=ll,ymax=ul), alpha=0.3) +
  geom_line(size=1) +
  ylab("Probability of borrowing") +
  xlab("Frequency") +
  coord_cartesian(ylim=c(0,1))

px = plot_smooth(m0.obj,view="AoA_objscaled", rm.ranef = T, print.summary=F)
px$fv$fit = logit2per(px$fv$fit)
px$fv$ul = logit2per(px$fv$ul)
px$fv$ll = logit2per(px$fv$ll)
px$fv$AoA_objscaled = px$fv$AoA_objscaled * attr(dataloan2$AoA_objscaled,"scaled:scale") + attr(dataloan2$AoA_objscaled,"scaled:center")

gAoAO = ggplot(px$fv, aes(x=AoA_objscaled,y=fit)) + 
  geom_ribbon(aes(ymin=ll,ymax=ul), alpha=0.3) +
  geom_line(size=1) +
  ylab("Probability of borrowing") +
  xlab("Age of acquisition (objective)")+
  coord_cartesian(ylim=c(0,1))

px = plot_smooth(m0.obj,view="phonlengthscale", rm.ranef = T, print.summary=F)
px$fv$fit = logit2per(px$fv$fit)
px$fv$ul = logit2per(px$fv$ul)
px$fv$ll = logit2per(px$fv$ll)
px$fv$phonlengthscale = px$fv$phonlengthscale * phonlength.scale + phonlength.center

gLenO = ggplot(px$fv, aes(x=phonlengthscale,y=fit)) + 
  geom_ribbon(aes(ymin=ll,ymax=ul), alpha=0.3) +
  geom_line(size=1) +
  ylab("Probability of borrowing") +
  xlab("Word length")+
  coord_cartesian(ylim=c(0,1))
```

```{r echo=F}
grid.arrange(gLenO, gFreqO, gAoAO, nrow=1)
pdf("../results/graphs/English_ModelResults_ObjectiveAoA_NoRF.pdf",
    height =3,width = 8)
grid.arrange(gLenO, gFreqO, gAoAO, nrow=1)
dev.off()

```


Note that concreteness is marginally significant in this model.  However, the trend is weak, and the decrease is only significant (according to the derivitives test) for a small section of the range:

```{r fig.show="hide"}
px = plot_smooth(m0.obj,view="concscale", rm.ranef = T, print.summary=F)
px$fv$fit = logit2per(px$fv$fit)
px$fv$ul = logit2per(px$fv$ul)
px$fv$ll = logit2per(px$fv$ll)
px$fv$phonlengthscale = px$fv$concscale * conc.scale + conc.center
```

```{r}
gConcO = ggplot(px$fv, aes(x=concscale,y=fit)) + 
  geom_ribbon(aes(ymin=ll,ymax=ul), alpha=0.3) +
  geom_line(size=1) +
  ylab("Probability of borrowing") +
  xlab("Concreteness")+
  coord_cartesian(ylim=c(0,1))
gConcO

plotGAMSignificantSlopes(m0.obj,'concscale','Concreteness',aoaLab = "AoA_objscaled")
```


```{r echo=F, eval=F}
# Old code for making graphs without controlling for random effects
pxobj = plot.gam(m0.obj,select=1, xlab="Word length", ylab="Log odds of borrowing",shade = T)

y = logit2per(pxobj[[1]]$fit)
x = pxobj[[1]]$x *phonlength.scale + phonlength.center
se.upper = logit2per(pxobj[[1]]$fit+pxobj[[1]]$se)
se.lower = logit2per(pxobj[[1]]$fit-pxobj[[1]]$se)
dx = data.frame(x=x,y=y,ci.upper=se.upper,ci.lower=se.lower)
plenobj = ggplot(dx, aes(x=x,y=y))+
  geom_ribbon(aes(ymin=ci.lower,ymax=ci.upper), alpha=0.3)+
  geom_line(size=1) +
  xlab("Word Length")+
  ylab("Probability of borrowing")+
  coord_cartesian(ylim = c(0,1))
paoaobj = rescaleGam(pxobj,2,dataloan2$AoAscale, "Age of acquisition")
pfreqobj = rescaleGam(pxobj,3,dataloan2$subtlexzipfscale, "Frequency")
pconcobj = rescaleGam(pxobj,4,dataloan2$concscale, "Concreteness")
pconcobj
grid.arrange(plenobj,pfreqobj,paoaobj, nrow=1)
```


# Comparison to Swadesh list words

Match up words with presence or absence in Swedesh list

```{r}
dx = dataloan2[,      c("subtlexzipf",
                        "AoA",
                        "phonlength",
                        "conc",
                        "Swadesh"),]
names(dx) = c("Frequency","AoA","Length","Concreteness","Swadesh")
dx$Swadesh = c("No","Yes")[1+as.numeric(dx$Swadesh)]
dx = dx[complete.cases(dx),]

```

Plot the distributions (code hidden but available in the Rmd file):

```{r echo=F}
dFreq = ggplot(dx,aes(Frequency, fill=Swadesh)) +
  geom_density(alpha=0.6) + theme_minimal() +
  scale_fill_manual(values = c(gray(0.1),gray(0.8))) +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        panel.grid = element_blank(),
        legend.position = 'none',
        plot.margin=unit(c(1,0.1,0.2,0.1),"cm"))
dAoA = ggplot(dx,aes(AoA, fill=Swadesh)) +
  geom_density(alpha=0.6) + theme_minimal() +
  scale_fill_manual(values = c(gray(0.2),gray(0.8))) +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        panel.grid = element_blank(),
        legend.position = 'none')
dLen = ggplot(dx,aes(Length, fill=Swadesh)) +
  geom_density(bw=1,alpha=0.6) + theme_minimal() +
  scale_fill_manual(values = c(gray(0.2),gray(0.8)),
                    breaks = c("Yes","No"),
                    labels = c("Swadesh list words","Wider lexicon")) +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        panel.grid = element_blank(),
        legend.title = element_blank(),
        legend.position = 'top')
dConc = ggplot(dx,aes(Concreteness, fill=Swadesh)) +
  geom_density(alpha=0.6) + theme_minimal() +
  scale_fill_manual(values = c(gray(0.2),gray(0.8))) +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        panel.grid = element_blank(),
        legend.position = 'none')

grid.arrange(dLen,dFreq,dAoA,dConc, nrow=2) 
pdf("../results/graphs/English_Distribution.pdf",
    width= 5.5, height = 4)
grid.arrange(dLen,dFreq,dAoA,dConc, nrow=2) 
dev.off()
```

Try a PCA plot:

```{r}

dx = dataloan2[,      c("subtlexzipfscale",
                        "AoAscale",
                        "phonlengthscale",
                        "concscale",
                        "Swadesh"),]
names(dx) = c("Frequency","AoA","Length","Conc.","Swadesh")
dx$Swadesh = c("No","Yes")[1+as.numeric(dx$Swadesh)]
dx = dx[complete.cases(dx),]

pc = prcomp(dx[, 1:4])

gpc1 = autoplot(pc, data = dx, colour = 'Swadesh',
         loadings = TRUE, loadings.colour = 'black', loadings.label.colour='black',
         loadings.label = TRUE, loadings.label.size = 5,frame = TRUE, frame.type = 'norm') +
  theme(legend.position = 'none')

gpc2 = autoplot(pc, data = dx, x = 1,y = 3,
         colour = 'Swadesh',
         loadings = TRUE, loadings.colour = 'black', loadings.label.colour='black',
         loadings.label = TRUE, loadings.label.size = 5,frame = TRUE, frame.type = 'norm') 
gxpc123 = grid.arrange(gpc1, gpc2, nrow=1, widths=c(0.8,1))
pdf("../results/graphs/English_Swadesh_PCA.pdf",
    width = 10, height= 4.5)
plot(gxpc123)
dev.off()
```

## Borrowing factors in Swadesh vs non-Swadesh words

First, we make a new model with a fixed effect for whether the word is in the Swadesh list:

```{r}
dataloan2$Swadesh=factor(dataloan2$Swadesh)
mSwadesh = update(m0,~.+Swadesh)
```

Now add an interaction for a word's length and whether it's in the Swadesh list.  Then test whether the fit to the data significantly improves.

```{r}
mSwadeshLen = update(mSwadesh,~.+
      s(phonlengthscale,by=Swadesh))
lrtest(mSwadesh,mSwadeshLen)
```

Same for AoA:

```{r}
mSwadeshAoA = update(mSwadesh,~.+
      s(AoAscale,by=Swadesh))
lrtest(mSwadesh,mSwadeshAoA)
```

Same for Frequency:

```{r}
mSwadeshFreq = update(mSwadesh,~.+
      s(subtlexzipfscale,by=Swadesh))
lrtest(mSwadesh,mSwadeshFreq)
```

Same for Concreteness:

```{r}
mSwadeshConc = update(mSwadesh,~.+
      s(concscale,by=Swadesh))
lrtest(mSwadesh,mSwadeshConc)
```

The model is not improved by adding an interaction between any predictor and whether a word is in the Swadesh list.  That is, the relationship betwen the predictors and borrowing is not significantly different for Swadesh words versus non-Swadesh words. 

\newpage

# Sensitivity analyses

## Individual models for each variable

Check that the response function is similar when including a variable alone in a model.  Run seperate models with single predictors:

```{r}
m0.length = bam(bor15.cat ~
      s(phonlengthscale)+
      s(cat,bs='re')+
      s(cat,phonlengthscale,bs='re'),
    data = dataloan2,
    family='binomial')

m0.AoA = bam(bor15.cat ~
      s(AoAscale)+
      s(cat,bs='re')+
      s(cat,AoAscale,bs='re'),
    data = dataloan2,
    family='binomial')

m0.frequency = bam(bor15.cat ~
      s(subtlexzipfscale)+
      s(cat,bs='re')+
      s(cat,subtlexzipfscale,bs='re'),
    data = dataloan2,
    family='binomial')

m0.conc = bam(bor15.cat ~
      s(concscale)+
      s(cat,bs='re')+
      s(cat,concscale,bs='re'),
    data = dataloan2,
    family='binomial')
```

Plot the original curves against the single-variable model curves:

```{r}
par(mfrow=c(2,2), mar=c(1,2,3,1))
mx = list(m0.length, m0.AoA, m0.frequency,m0.conc)
mx.labels = c("Length","AoA","Frequency","Concreteness")
for(i in 1:4){
plot(m0,select=i,main=mx.labels[i],ylim=c(-4,4))
par(new=T)
plot(mx[[i]],select=1,ylim=c(-4,4), col=2,ylab="")
legend(-1,-1.5,legend = c("Original","Single variable"),col=1:2,lty=1, bty='n')
}
par(mfrow=c(1,1))
```

Compare the EDF values.

```{r}
orig.edf = summary(m0)$edf[1:4]
single.edf = lapply(mx,function(X){summary(X)$edf[1]})
plot(orig.edf,
     single.edf,
     xlab="Original EDF",
     ylab="Single variable EDF",
     ylim=c(0,3),xlim=c(0,3.5),
     main="Compare EDF values")
text(orig.edf,single.edf,mx.labels,pos=2)
abline(0,1,lty=2)

```

The results for length and AoA are almost identical.  The results for frequency are similar (non-linear relationship with a peak in the middle), although the significant slope is now for the higher values:

```{r}
plotGAMSignificantSlopes(m0.frequency,"subtlexzipfscale","Frequency")
```

By itself, concreteness is a signficant predictor.

```{r}
plotGAMSignificantSlopes(m0.conc,"concscale","Concreteness")
```



## GAM with PoS as fixed effects

Part of speech was modelled above as a random effect.  Here we show that the estimates differ very little if we treat part of speech as a fixed effect:

```{r}
m0.posFE = bam(bor15.cat ~
      s(phonlengthscale) + 
      s(AoAscale) + 
      s(subtlexzipfscale) +
      s(concscale) +
      cat,
    data = dataloan2,
    family='binomial')
```

```{r}
vars = c("phonlengthscale","AoAscale","subtlexzipfscale","concscale")
varLabels = c("Length","AoA","Frequency","Concreteness")

for(i in 1:4){
  plot(m0,select=i,main=varLabels[i],ylim=c(-4,4))
  par(new=T)
  plot(m0.posFE,select=i,ylim=c(-4,4), col=2)
  legend(0,-2,legend = c("Original","PoS Fixed"),col=1:2,lty=1)
}
```


Test the inclusion of interactions between cat and other fixed effects by log ratio tests:

```{r}
m0.posFE.catByLen = update(m0.posFE,~.+s(phonlengthscale,by=cat))
lrtest(m0.posFE,m0.posFE.catByLen)
```

Adding an interaction between part of speech and length results in a better fit.  However, since there is a realtively small range of unique length values, and that many part of speech categories have very limited length ranges, then perfect seperation occurs and the model has poor convergence.  The results of the model suggest that length has a bigger effect size for some parts of speech than others.  This is better captured by a mixed effects model.  The estimates for other fixed effects are not qualitatively different in this model.


cat x AoA is not significant:

```{r}
m0.posFE.catByAoA = update(m0.posFE.catByLen,~.+s(AoAscale,by=cat))
lrtest(m0.posFE.catByLen,m0.posFE.catByAoA)
```

cat x frequency is not significant:

```{r}
m0.posFE.catByFreq = update(m0.posFE.catByLen,~.+s(subtlexzipfscale,by=cat))
lrtest(m0.posFE.catByLen,m0.posFE.catByFreq)
```

cat x concreteness is not significant:

```{r}
m0.posFE.catByConc = update(m0.posFE.catByLen,~.+s(concscale,by=cat))
lrtest(m0.posFE.catByLen,m0.posFE.catByConc)
```

\newpage

## Controlling for source language length

We attempted to obtain estimates for the distribution of word lengths in source languages (for the English dataset). This is very difficult, because there are 25 source languages from across the world, and very few of these have large word list sources. The only possibility is the Automatic Similarity Judgement Program database, which contains a small standard list of words for many languages. We obtained these words for the source languages (subsituting phylogenetically close languages for those with no data, words which were not borrowed were matched to modern English data from the ASJP).  We calculated the average word length for each source language. See the file `processing/GetSourceLanguageWordLengths.R` for details.

Compare the original model with a model with control for average donor langauge word length.

```{r warning=F}
mMeanLenControl = bam(bor15.cat ~
      s(phonlengthscale) + 
      s(AoAscale) + 
      s(subtlexzipfscale) +
      s(concscale) +
        s(SLMWL,k=3) +
      s(cat,bs='re')+
      s(cat,phonlengthscale,bs='re')+
      s(cat,AoAscale,bs='re')+
      s(cat,subtlexzipfscale,bs='re')+
      s(cat,concscale,bs='re'),
    data = dataloan2,
    family='binomial')

summary(mMeanLenControl)
```

The original length term is still significant. Original length term curve:

```{r}
plotGAMSignificantSlopes(mMeanLenControl,
            "phonlengthscale",'phonlengthscale')
```

Relationship between mean word length in donor language and probaility of borrowing:

```{r}
plotGAMSignificantSlopes(mMeanLenControl,
            "SLMWL",'SLMWL')
```


```{r}
mx.labels = c("Length","AoA","Frequency","Concreteness")
par(mfrow=c(2,2),mar=c(2,2,1,2))
for(i in 1:4){
plot(m0,select=i,main=mx.labels[i],ylim=c(-4,4))
par(new=T)
plot(mMeanLenControl,select=i,ylim=c(-4,4), col=2,ylab="")
legend(-1,-1.5,legend = c("Original","With length control"),col=1:2,lty=1, bty='n')
}
par(mfrow=c(1,1))
```

\newpage

We also attempted to calculate the measure that the reviewer suggested: the frequency of the length of the given borrowed word in each donor language. This was done with the ASJP data. For each borrowed word, we calculated the frequency of having a word of that length in the source language. The word lists are small, so not all word lengths are represented. Missing values were imputed by regression. Frequency estimates for non-borrowed words came from estimates from the ASJP for modern English.

Use the source word length frequency measure to control for borrowing from langauges with longer words.

```{r}
mLenFreqControl = update(m0, ~.+s(SWF,k=3))
summary(mLenFreqControl)
plot(mLenFreqControl,select=10)
```

Compare the original model with a model with control for donor language length.

```{r}
mx.labels = c("Length","AoA","Frequency","Concreteness")
par(mfrow=c(2,2),mar=c(2,2,1,2))
for(i in 1:4){
plot(m0,select=i,main=mx.labels[i],ylim=c(-10,10))
par(new=T)
plot(mLenFreqControl,select=i,ylim=c(-10,10), col=2,ylab="")
legend(-1,-1.5,legend = c("Original","With length control"),col=1:2,lty=1, bty='n')
}
par(mfrow=c(1,1))
```

